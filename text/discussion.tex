\section{Discussion}
\label{sec:discussion}

This paper introduced new classes of shrinkage estimators for
tensor-valued data that are higher-order generalizations of existing
matrix spectral estimators. Each class is indexed by tuning parameters
whose values we chose by minimizing an unbiased estimate of the
risk. In terms of MSE, these estimators outperform their matrix
counterparts when the mean has approximately low multilinear rank and
they perform competitively when the mean does not have low multilinear
rank.

There has been some recent work on penalized optimization methods for
estimating signal tensors in the presence of Gaussian noise
\citep{signoretto2010convex,tomioka2011estimation,tomioka2011statistical,liu2013tensor,tomioka2013convex}. Usually,
these estimators are defined as the minimizers of a penalized squared
error empirical loss, where the penalty is usually some generalization
of the nuclear norm to tensors (for example, the sum of the nuclear
norms of the $K$ matricizations of a tensor). These estimators, though
similar in spirit, are very different from our approach. The main
advantage of our estimators is their simplicity --- they are simply
functions of the HOSVD (\ref{equation:ho.spect.est}) for which there
are efficient and accurate numerical procedures to compute.

We have presented a way to adaptively choose the tuning parameters of
our higher-order spectral estimators by minimizing the SURE. This
approach is applicable, not just for the truncated HOSVD
(\ref{equation:trunc.shrink}) and the mode-specific soft-thresholding
(\ref{equation:msst.est}) estimators, but also for \emph{all}
estimators of the form (\ref{equation:ho.spect.est}) that satisfy the
conditions of Theorem \ref{theorem:stein.theorem}.  Although we found
that adaptively choosing the tuning parameters by minimizing the SURE
worked well under the scenarios we studied, there are other ways to
select tuning parameters. In the case of matrix spectral estimators,
others have chosen the amount of shrinkage by minimax considerations
\citep{efron1972empirical,stein1981estimation}, cross-validation
\citep{bro2008cross,owen2009bi,josse2012selecting}, and asymptotic
considerations
\citep{gavish2014optimalhard,gavish2014optimal}. Exploring these
methods for our higher-order spectral estimators
(\ref{equation:ho.spect.est}) is a current research area of the
authors.

In this paper, we focused on estimators of the form
(\ref{equation:ho.spect.est}). If the mean tensor is believed to have
approximately low multilinear rank, we should shrink the core array
through the Tucker product along the modes to obtain this low
multilinear rank. The form of our higher-order spectral estimators
(\ref{equation:ho.spect.est}) allows us to use the mode-specific
singular values to determine the form and amount of shrinkage that
should be performed to each mode of the core array. However, different
classes of higher-order spectral estimators can be studied. In the
Appendix \ref{sec:sure.s.shrink}, we explore functions that shrink
each element of the core array individually:
\begin{align*}
t(\mathcal{X}) = (U_1,\ldots,U_K)\cdot g(\mathcal{S}), \text{ where } g(\mathcal{S})_{[\mathbf{i}]} = g_{\mathbf{i}}(\mathcal{S}_{[\mathbf{i}]}).
\end{align*}
This class of estimators can be used, for example, to induce zeros in
the core array, which has applications in increasing the
interpretability of a higher-order generalization of principal
components analysis
\citep{henrion1993body,kiers1997uniqueness,murakami1998case,andersson1999general,de2001independent,martin2008jacobi}.

Although the error variance $\tau^2$ in (\ref{equation:normal.model})
might be known in some settings, such as fMRI data sets
\citep{candes2013unbiased}, in most applied situations the variance
would not be unknown. There are matrix-specific estimates of the
variance that can be applied to tensor-variate datasets by first
matricizing along each mode. In our software, we have implemented the
methods described in \citet{choi2014selecting} and
\citet{gavish2014optimalhard}. Though, instead of plugging in an
estimate of the variance into the SURE formula
(\ref{equation:sure.form}), there has been a recent suggestion to use
a generalized SURE formula
\citep{sylvain2012smooth,josse2015adaptive}:
\begin{align*}
\gsure(t) = \frac{||t(\mathcal{X}) - \mathcal{X}||^2}{(1 - \diverge(t(\mathcal{X}))/p)^2}.
\end{align*}
This formula is motivated by generalized cross-validation
\citep{golub1979generalized} and is an approximation to SURE
\citep{josse2015adaptive}. Importantly, GSURE does not require the
variance to be known, and so its minimization may be accomplished
without an estimate of $\tau^2$. For our higher-order spectral
estimators, we have already accomplished the hard work of calculating
the divergence in this paper, and implementing GSURE is an easy
application of this result. Our software allows for GSURE
implementation for the estimators discussed in this article.


All methods discussed in this paper are implemented in the R package
\texttt{hose} available at
\begin{center}
  \href{https://github.com/dcgerard/hose}{https://github.com/dcgerard/hose}.
\end{center}
Code and instructions to reproduce all of the results of this paper are available at
\begin{center}
\href{https://github.com/dcgerard/hose\_paper/tree/master/reproduce\_sure}{https://github.com/dcgerard/hose\_paper/tree/master/reproduce\_sure}.
\end{center}