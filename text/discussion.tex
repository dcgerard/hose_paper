\section{Discussion}
\label{sec:discussion}

This paper introduced new classes of shrinkage estimators for
tensor-valued data that are higher-order generalizations of existing
matrix spectral estimators. Each class is indexed by tuning parameters
whose values we chose by minimizing an unbiased estimate of the
risk. In terms of MSE, these estimators outperform their matrix
counterparts when the mean has approximately low multilinear rank and
they perform competitively when the mean does not have low multilinear
rank.

There has been some recent work on penalized optimization methods for
estimating signal tensors in the presence of Gaussian noise
\citep{signoretto2010convex,tomioka2011estimation,tomioka2011statistical,liu2013tensor,tomioka2013convex}. Usually,
these estimators are defined as the minimizers of a penalized squared
error empirical loss, where the penalty is usually some generalization
of the nuclear norm to tensors (for example, the sum of the nuclear
norms of the $K$ matricizations of a tensor). These estimators, though
similar in spirit, are very different from our approach. Our
estimators can be written in closed form (\ref{equation:ho.spect.est})
and are not the solution of a minimization problem. While both our
class of estimators and the class of penalized optimization estimators
contain tuning parameters, the penalized optimization approaches are
not totally adaptive as they do not provide methods to select the
tuning parameters. We have presented a way to adaptively choose the
tuning parameters of our higher-order spectral estimators by
minimizing the SURE. This approach is applicable, not just for the
truncated HOSVD (\ref{equation:trunc.shrink}) and the mode-specific
soft-thresholding (\ref{equation:msst.est}) estimators, but also for
\emph{all} estimators of the form (\ref{equation:ho.spect.est}) that
satisfy the conditions of Theorem \ref{theorem:stein.theorem}.

Although we found that adaptively choosing the tuning parameters by
minimizing the SURE worked well under the scenarios we studied, there
are other ways to select tuning parameters. In the case of matrix
spectral estimators, others have chosen the amount of shrinkage by
minimax considerations \citep{efron1972empirical,stein1981estimation},
cross-validation \citep{bro2008cross,owen2009bi,josse2012selecting},
and asymptotic considerations
\citep{gavish2014optimalhard,gavish2014optimal}. Exploring these
methods for our higher-order spectral estimators
(\ref{equation:ho.spect.est}) is a current research area of the
authors.

In this paper, we focused on estimators of the form
(\ref{equation:ho.spect.est}). If the mean tensor is believed to have
approximately low multilinear rank, we should shrink the core array
through the Tucker product along the modes to obtain this low
multilinear rank. The form of our higher-order spectral estimators
(\ref{equation:ho.spect.est}) allows us to use the mode-specific
singular values to determine the form and amount of shrinkage that
should be performed to each mode of the core array. However, different
classes of higher-order spectral estimators can be studied. In the
Appendix \ref{sec:sure.s.shrink}, we explore functions that shrink
each element of the core array individually:
\begin{align*}
t(\mathcal{X}) = (U_1,\ldots,U_K)\cdot g(\mathcal{S}), \text{ where } g(\mathcal{S})_{[\mathbf{i}]} = g_{\mathbf{i}}(\mathcal{S}_{[\mathbf{i}]}).
\end{align*}
This class of estimators can be used, for example, to induce zeros in
the core array, which has applications in increasing the
interpretability of a higher-order generalization of principal
components analysis
\citep{henrion1993body,kiers1997uniqueness,murakami1998case,andersson1999general,de2001independent,martin2008jacobi}.

Although the error variance $\tau^2$ in (\ref{equation:normal.model})
might be known in some settings, such as fMRI data sets
\citep{candes2013unbiased}, in most applied situations the variance
would not be unknown. Instead of plugging in an estimate of the
variance into the SURE formula (\ref{equation:sure.form}), there has
been a recent suggestion to use a generalized SURE formula
\citep{sylvain2012smooth,josse2015adaptive}:
\begin{align*}
\gsure(t) = \frac{||t(\mathcal{X}) - \mathcal{X}||^2}{(1 - \diverge(t(\mathcal{X}))/p)^2}.
\end{align*}
This formula is motivated by generalized cross-validation
\citep{golub1979generalized} and is an approximation to SURE
\citep{josse2015adaptive}. Importantly, GSURE does not require the
variance to be known, and so its minimization may be accomplished
without an estimate of $\tau^2$. For our higher-order spectral
estimators, we have already accomplished the hard work of calculating
the divergence in this paper, and implementing GSURE is an easy
application of this result. The code available on the first author's
web page allows for GSURE implementation for the estimators discussed
in this article.


All methods discussed in this paper are available in the R package
\texttt{hose} available at
\begin{center}
  \href{https://github.com/dcgerard/hose}{https://github.com/dcgerard/hose}.
\end{center}