\section{The higher-order SVD and higher-order spectral estimators}
\label{sec:tensor}

Some tensor data sets have approximately low \emph{multilinear rank},
which we now define. Recall that the rank of a matrix is the dimension
of the vector space spanned by its columns and rows. Define the
$k$-mode vectors of a tensor $\mathcal{X} \in
\mathbb{R}^{p_1\times\cdots\times p_K}$ as the $p_k$-dimensional
vectors formed from $\mathcal{X}$ by varying $i_k$ and keeping the
other indices fixed. The $k$-mode rank $r_k$ is the dimension of the
span of the $k$-mode vectors, and the multilinear rank of the
$K$-order tensor $\mathcal{X}$ is the $K$-tuple,
$(r_1,\ldots,r_K)$. Define the $k$-mode matricization
\citep{kolda2009tensor}, or $k$-mode unfolding, of $\mathcal{X}$ to be
$\mathcal{X}_{(k)}\in \mathbb{R}^{p_k \times p/p_k}$ (with $p =
\prod_{k=1}^Kp_k$) where element $(i_1,\ldots,i_K)$ in $\mathcal{X}$
maps to element $(i_k,j)$ in $\mathcal{X}_{(k)}$ where
\begin{align*}
  j = 1 + \sum_{\substack{n = 1 \\ n\neq k}}^{K}(i_n - 1)J_n \text{ with } J_n = \prod_{\substack{ m = 1 \\ m \neq k}}^{n-1}p_m.
\end{align*}
Then, equivalently, $r_k$ is the rank of $\mathcal{X}_{(k)}$.

The SVD , presented in Section \ref{sec:intro}, has been used to
shrink matrix valued data towards low rank. One generalization of the
SVD to tensors is the HOSVD of \cite{de2000multilinear}, which relates
directly to multilinear rank.

\begin{definition}[HOSVD of \cite{de2000multilinear}]
  Let $\mathcal{X}_{(k)} = U_kD_kV_k^T$ be the SVD of each $k$-mode
  unfolding of $\mathcal{X}$. Let $\mathcal{S} =
  (U_1^T,\ldots,U_K^T)\cdot\mathcal{X}$, then
  \begin{align}
    \label{equation:hosvd}
    \mathcal{X} = (U_1,\ldots,U_K)\cdot \mathcal{S}
  \end{align}
  is the higher-order singular value decomposition (HOSVD).
\end{definition}
The product ``$\cdot$'' in (\ref{equation:hosvd}) between a list of
matrices, $\{U_1,\ldots,U_K\}$ for $U_k \in \mathbb{R}^{p_k \times
  p_k}$, and a tensor, $\mathcal{S} \in
\mathbb{R}^{p_1\times\cdots\times p_K}$ is called the \emph{Tucker
  product}. The Tucker product is defined through the $k$-mode
matricizations of $(U_1,\ldots,U_K)\cdot \mathcal{S}$:
\begin{align*}
  \begin{split}
    &\mathcal{X} = (U_1,\ldots,U_K)\cdot \mathcal{S}\\
    &\Leftrightarrow \mathcal{X}_{(k)} = U_k\mathcal{S}_{(k)}(U_K^T\otimes\cdots\otimes U_{k+1}^T\otimes U_{k-1}^T\otimes\cdots\otimes U_1^T) = U_k\mathcal{S}_{(k)}U_{-k}^T,
  \end{split}
\end{align*}
where ``$\otimes$'' is the Kronecker product.  The ``core array'',
$\mathcal{S}$ has the property of \emph{all-orthogonality} where
\begin{align*}
  \mathcal{S}_{(k)}\mathcal{S}_{(k)}^T = D_k^2 \text{ for all } k = 1,\ldots,K.
\end{align*}

The HOSVD is multilinear rank-revealing in the same way the SVD is
rank-revealing. That is, let $D_k =
(\mathcal{S}_{(k)}\mathcal{S}_{(k)}^T)^{1/2} =
\diag(\sigma_1^k,\ldots,\sigma_{p_k}^k)$ be the mode specific singular
values of $\mathcal{X}$. Then the multilinear rank of $\mathcal{X}$ is
$(r_1,\ldots,r_K)$ if $D_k$ contains $r_k$ non-zero mode-specific
singular values. In the core array, this is equivalent to
$\mathcal{S}$ containing zeros everywhere except in one of the
``corners'': $\mathcal{S}_{[1:r_1,\ldots,1:r_K]}$, where $1:r_k =
1,\ldots,r_k$. It is possible, then, to shrink $\mathcal{S}$ towards
having (approximately) low multilinear rank by shrinking the elements
in $\mathcal{S}$ towards $0$. We propose doing this via a
re-parameterization of $\mathcal{S}$, given as follows:
\begin{align}
  \label{equation:hosvd.rewrite}
  \begin{split}
    \mathcal{X} = (U_1,\ldots,U_K) \cdot (D_1,\ldots,D_K)\cdot (D_1^{-1},\ldots,D_K^{-1})\cdot \mathcal{S}\\
    = (U_1,\ldots,U_K) \cdot (D_1,\ldots,D_K)\cdot \mathcal{V},
  \end{split}
\end{align}
where $\mathcal{S} = (D_1,\ldots,D_K)\cdot \mathcal{V}$. Our
higher-order spectral estimators shrink $\mathcal{S}$ by shrinking
each mode-specific $D_k$. We abuse notation a little by allowing
``$\cdot$'' to also represent a binary operator between two lists of
matrices whose operation is component-wise multiplication. This should
not cause confusion because $(A_1B_1,\ldots,A_KB_K) \cdot \mathcal{C}
= (A_1,\ldots,A_K) \cdot [(B_1,\ldots,B_K) \cdot \mathcal{C}]$.

Using reparameterization (\ref{equation:hosvd.rewrite}), we now define
higher-order spectral estimators of $\Theta$ under the model
(\ref{equation:normal.model}).
\begin{definition}
  Let $\mathcal{X} = (U_1,\ldots,U_K) \cdot (D_1,\ldots,D_K)\cdot \mathcal{V}$ as in (\ref{equation:hosvd.rewrite}) with $D_k = \diag(\sigma_1^k,\ldots,\sigma_{p_k}^k)$. An estimator $t(\mathcal{X})$ of the form
  \begin{align}
    \label{equation:ho.spect.est}
    t(\mathcal{X}) = (U_1,\ldots,U_K) \cdot (f^1(D_1),\ldots,f^K(D_K))\cdot \mathcal{V},
  \end{align}
  where $f^k(D_k) =
  \diag(f_1^k(\sigma_1^k),\ldots,f_{p_k}^k(\sigma_{p_k}^k))$, is
  called a \emph{higher-order spectral estimator}.
\end{definition}

Each of the matrix shrinkage functions listed in Section
\ref{sec:intro}
(\ref{equation:eckart.young})-(\ref{equation:josse.sardy.est}) may, in
principle, be applied to each mode in our higher-order spectral
estimator (\ref{equation:ho.spect.est}). We focus on two examples of
higher-order spectral estimators. One of these is a generalization of
the matrix truncation estimator (\ref{equation:eckart.young}) and the
other is a generalization of the matrix soft-thresholding estimator
(\ref{equation:soft.thresholding}). The former can be used to choose
the multilinear rank of $\Theta$, the latter is for estimation of
$\Theta$ when we suspect that the mean tensor has approximately low
multilinear rank.
