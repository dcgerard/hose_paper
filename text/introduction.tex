\section{Introduction}
\label{sec:intro}

Tensor data arise in fields as diverse as relational data
\citep{hoff2015multilinear}, neuroimaging
\citep{zhang2014tensor,li2016parsimonious}, psychometrics
\citep{kiers2001three}, chemometrics
\citep{smilde2005multi,bro2006review}, signal processing
\citep{cichocki2015tensor}, and machine learning
\citep{tao2005supervised}, among others
\citep{kroonenberg2008applied}.
% In particular, tensor decompositions \citep{kolda2009tensor}, as a
% way of identifying tensor structure, are being applied to each of
% these fields.
A tensor $\mathcal{X} \in \mathbb{R}^{p_1\times\cdots\times p_K}$ with
$p_k \in \{1,2,\ldots\}$ of order $K$ is a $K$-way array where the
elements $\mathcal{X}_{[i_1,\ldots,i_K]}$ are indexed by $i_k \in
\{1,2,\ldots,p_k\}$ for $k = 1,\ldots,K$. For example, a multivariate
relational dataset can be expressed as a tensor, where element
$\mathcal{X}_{[i,j,t]}$ of the tensor is the $t$th relation between
actors $i$ and $j$.

Often, a tensor is corrupted by noise. The model we consider for this
is:
\begin{align}
  \label{equation:normal.model}
  \mathcal{X} = \Theta + \mathcal{E},\text{  } \mathcal{E}_{[i_1,\ldots,i_K]} \sim N(0,\tau^2) \text{ independent for } i_k = 1,\ldots,p_k, \text{ and } k = 1,\ldots,K,
\end{align}
where $\Theta \in \mathbb{R}^{p_1\times\cdots\times p_K}$ is the
signal and $\mathcal{E} \in \mathbb{R}^{p_1\times\cdots\times p_K}$ is
the additive Gaussian measurement error or noise with mean 0 and
various $\tau^2$. The performance of an estimator
$t(\mathcal{X})\in \mathbb{R}^{p_1\times\cdots\times p_K}$ can be
evaluated by statistical risk under quadratic loss, i.e.\ mean squared
error (MSE):
\begin{align}
  \label{equation:mse}
  \mse(t(\mathcal{X})) = E_{\Theta}[||\Theta - t(\mathcal{X})||^2] = \sum_{\mathbf{i}} E_{\Theta}[(\Theta_{[\mathbf{i}]} - t(\mathcal{X})_{[\mathbf{i}]})^2],
\end{align}
where $\mathbf{i} = (i_1,\ldots,i_K)$ is a $K$-tuple of tensor
indices.

In the matrix variate case, $X \in \mathbb{R}^{p \times n}$, an
investigator often believes that the mean is well approximated by a
low rank matrix. There has been much work on ``denoising'' (or mean
estimation) in matrix variate data by using this knowledge. A typical
estimation scheme begins by computing the singular value decomposition
(SVD) of $X$:
\begin{align}
  \label{equation:svd}
  X = UDV^T,
\end{align}
where, in the case $n \geq p$, $U \in \mathbb{R}^{p \times p}$ is
orthogonal, $D = \diag(\sigma_1,\ldots,\sigma_p)$ with $\sigma_1 \geq
\ldots\geq \sigma_p \geq 0$, and $V \in \mathbb{R}^{n \times p}$
contains orthonormal columns. The columns of $U$ and $V$ are,
respectively, the left and right singular vectors of $X$ and the
diagonal elements of $D$ are the singular values. A key property of
the SVD is that the number of non-zero singular values of $X$ is
precisely the rank of $X$. One widely studied approach to estimating
$\Theta$ when it is assumed that $\Theta$ has nearly low rank is to
shrink the singular values of $X$ towards $0$ while keeping the
singular vectors unchanged, thereby inducing an (approximately) low
rank estimate. The resulting ``spectral'' estimator $t(\mathcal{X})$
of $\Theta$ then takes the form $t(\mathcal{X}) = Uf(D)V^T$ where
$f(D) = \diag(f_1(\sigma_1),\ldots,f_K(\sigma_K))$ and each
$f_i(\cdot)$ shrinks the singular values towards $0$. These estimators
are orthogonally equivariant, meaning that $t(WXZ^T) = Wt(X)Z^T$ for
orthogonal matrices $W,Z$ \citep{shabalin2013reconstruction}.

Early work on singular value shrinkage estimation from a
non-statistical perspective began with \cite{eckart1936approximation},
where they proved that the best rank $r$ approximation to the data
matrix $X \in \mathbb{R}^{p \times n}$ (in terms of sum of squared
differences from $X$) is found with the shrinkage function:
\begin{align}
  \label{equation:eckart.young}
  f_i(\sigma_i) = \sigma_i 1(i \leq r),
\end{align}
where $1(\cdot)$ is the indicator function. We call
(\ref{equation:eckart.young}) the truncation estimator. However,
approximating the data $X$ well is not the same as estimating the
underlying signal $\Theta$ well. In terms of estimating $\Theta$, the
matrix $X$ is unbiased, minimax, and the maximum likelihood estimator
under normally distributed errors. However, it is well known that
shrinkage estimators, such at that of \cite{stein1981estimation} can
uniformly dominate $X$ in terms of risk. This seminal shrinkage
estimator, in the context of matrix estimation, is given by
\begin{align}
  \label{equation:stein}
  f_i(\sigma_i) = \left(1 - \frac{\lambda}{\sum_{i = 1}^p\sigma_i^2}\right)\sigma_i,
\end{align}
where $\lambda > 0$ is some tuning parameter. For data that exhibit
associations between the rows and/or columns of the mean matrix, the
estimator of \cite{efron1972empirical}, given by
\begin{align}
  \label{equation:efron.morris}
  f_i(\sigma_i) = \sigma_i - \frac{\lambda}{\sigma_i},
\end{align}
was introduced and results in different amounts of shrinkage for each
singular value.  \cite{efron1976multivariate} improved upon this
estimator with a generalization of both (\ref{equation:stein}) and
(\ref{equation:efron.morris}), given by
\begin{align}
  \label{equation:improved.em}
  f_i(\sigma_i) = \left(1 - \frac{\gamma}{\sum_{i = 1}^p\sigma_i^2}\right)\sigma_i - \frac{\lambda}{\sigma_i},
\end{align}
where $\lambda > 0$ and $\gamma > 0$ are tuning parameters.

More recent work has focused on estimators whose functions
$f_i(\cdot)$ induce sparsity in the singular values, which may be more
appropriate than (\ref{equation:stein}),
(\ref{equation:efron.morris}), and (\ref{equation:improved.em}) in
cases where the true signal itself has (approximately) low
rank. Motivated by penalized maximum likelihood estimation, the
hard-thresholding estimator
\begin{align}
  \label{equation:hard.thresholding}
  f_i(\sigma_i) = \sigma_i 1(\sigma_i \geq \lambda)
\end{align}
and the soft-thresholding estimator
\begin{align}
  \label{equation:soft.thresholding}
  f_i(\sigma_i) = (\sigma_i - \lambda)_+
\end{align}
were introduced \citep[for example]{candes2013unbiased}. Here, $(y)_+
= \max(y,0)$ is the ``positive part'' function. A clever shrinkage
function that includes (\ref{equation:hard.thresholding}),
(\ref{equation:soft.thresholding}), and a truncated version of
(\ref{equation:efron.morris}) \citep{verbanck2015regularised} as
special cases is that of \cite{josse2015adaptive}:
\begin{align}
  \label{equation:josse.sardy.est}
  f_i(\sigma_i) = \sigma_i\left(1 - \frac{\lambda^{\gamma}}{\sigma_i^{\gamma}}\right)_+.
\end{align}
This estimator was inspired by the adaptive LASSO
\citep{zou2006adaptive}. A variety of other shrinkage estimators have
also been developed
\citep{nadakuditi2014optshrink,shabalin2013reconstruction}.

All of these estimators are specific to matrix-variate data. If one
were to apply these matrix methods to a tensor, one would first
convert the tensor into a matrix. For a $K$-dimensional tensor, such
``matricization'' destroys the indexing structure along all but one of
the dimensions. This may be detrimental to estimation if, in addition
to a data set having approximately low rank, it also has approximately
low \emph{multilinear} rank (see Section \ref{sec:tensor}), that is,
``matricizing'' along each index set, or ``mode'', results in a low
rank matrix.

An extreme simulated example that exhibits this phenomenon is
presented in Figure \ref{fig:sim.extreme}. There, we plotted the
mode-specific singular values of a tensor that we generated to have
full rank along one mode and low ranks along two modes. That is, we
plotted the singular values of each matricization of the tensor. If an
analyst were presented with a noisy version of this tensor and only
matricizing along the first mode, then they would only observe a noisy
realization of the solid lines, which would suggest the data are full
rank. However, the second and third modes have low-rank structure and
shrinking the singular values along these additional modes may improve
estimation.


\begin{figure}
\begin{center}
\includegraphics{./fig/low_multilinear_rank.pdf}
\caption{Mode-specific singular values of simulated tensor with full
  rank along first mode and low-ranks along second and third modes.}
\label{fig:sim.extreme}
\end{center}
\end{figure}

% \textbf{New data showing low multilinear rank. Show scree plot for
%   each mode.} If we were only matricizing along the time dimension we
% would only observe the dotted lines, which seem to suggest the data
% have approximately rank 1. However, the vertical and horizontal
% dimensions also seem to have approximately low rank, as evidenced by
% the solid and dashed lines. Shrinking the singular values along these
% additional modes may also improve estimation.

%% In general, having low rank along one mode does not imply having
%% low rank along all modes. It is possible to have different
%% mode-specific ranks, and possible to shrink toward low multilinear
%% rank. For example, in the left plot of Figure \ref{fig:mario.movie}
%% we simulated a $10 \times 10 \times 10$ dimensional tensor data set
%% that contains full rank along the first mode and low rank along
%% modes 2 and 3. If we were only to observe the first mode matrix
%% unfolding, then we would not be aware of the low rank structure,
%% when in fact there is extremely low multilinear rank structure.




In this article, we introduce a family of estimators that shrink
tensor-valued data towards having (approximately) low multilinear
rank. We perform this shrinkage on a reparameterization of the
higher-order singular value decomposition (HOSVD) of
\cite{de2000multilinear}, where we shrink the mode-specific singular
values of the data tensor towards zero. We consider classes of such
``higher-order spectral estimators'', where a class is defined by a
mode-specific shrinkage function indexed by a tuning parameter. We
propose to adaptively select the tuning parameters by minimization of
an unbiased estimate of the risk.


%% Shrinkage estimators often contain tuning parameters whose
%% selection is difficult process. In the case of matrix spectral
%% estimators, others have chosen the amount of shrinkage by minimax
%% considerations \citep{efron1972empirical,stein1981estimation},
%% cross-validation
%% \citep{bro2008cross,owen2009bi,josse2012selecting}, asymptotic
%% considerations \citep{gavish2014optimalhard,gavish2014optimal}, and
%% recently, by minimizing an unbiased estimate of the MSE
%% \citep{candes2013unbiased,josse2015adaptive}. For our second
%% contribution, we extend this last method to the tensor case by
%% deriving Stein's unbiased risk estimate (SURE) for higher-order
%% spectral estimators.

Our paper is organized as follows. In Section \ref{sec:tensor}, we
review tensors and the HOSVD. We then present how one may define
functions that shrink the mode-specific singular values of the
HOSVD. In particular, we present two specific estimators that shrink
the data tensor towards having (approximately) low multilinear rank
and provide some discussion on the intuition behind these
estimators. In Section \ref{sec:sure_form}, we review Stein's unbiased
risk estimates (SURE), then derive the SURE for a broad class of
higher-order spectral estimators. In Section \ref{sec:simulation} we
present simulations demonstrating that (1) tensor specific methods
perform better when the mean tensor has approximately low multilinear
rank; (2) when the mean tensor has low multilinear rank our methods
accurately estimate the multilinear rank; and (3) tensor specific
methods perform competitively when the signal tensor does not have
approximately low multilinear rank. In Section \ref{sec:NBA} we
illustrate the use of these methods in an application to multivariate
relational data. We finish with a discussion in Section
\ref{sec:discussion}.
